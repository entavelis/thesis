% --------------------------------------------------------------
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL researchproposalold.tex   Sun Dec 17 12:51:50 2017
%DIF ADD researchproposal.tex      Sun Dec 17 12:48:27 2017
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{cite}
\graphicspath{ {../media/} } 

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFadd}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}

% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------

%\renewcommand{\qedsymbol}{\filledbox}

\title{Integrating vision and language using generative models: a research proposal}
\author{Evangelos Ntavelis\\ %replace with your name
r0692337\\
} %if necessary, replace with your course title

\maketitle

\section{Introduction}

There is evidence that the nature of human memory is reconstructive (Vernon 2014 \cite{ACS}; Hawkins and Blakeslee 2007\cite{onInt}). That means that our neural system breaks down the stimuli it receives from the outside world into an internal representation and it recreates it when the need arises. Moreover, Barsalou\cite{doi:10.1146/annurev.psych.59.103006.093639} argues that ground cognition draws upon the senses. It's normal, thus, to aim to computationally recreate this multi-modal decomposition, coding and composition procedure.

In the previous years we can observe a lot of approaches tackling this problem using auto-encoders, both combining audio and visual features (Ngiam et al, 2011 \cite{conf/icml/NgiamKKNLN11}) and visual and text features (Silberer and Lapata 2014 \cite{silberer2014learning}). The encoders were fed with pre-learned features independently for both modalities, which were then combined \textit{a posteriori}. 

More recently, a generative family of models, Generative Adversarial Networks, have gained popularity. Among other things, they were used as an alternative method to design multi-modal embeddings in video hyperlinking with promising results (Vukotic et al 2017).

\section{Goals}

In the course of this thesis project we aim to test how Generative Adversarial Networks Architectures perform in tasks of lingo-visual representations. Specifically, we want to:

 \begin{itemize} 
    \item Gauge the ability of these architectures to map informatively:
       \begin{itemize} 
        \item language to vision and back, (extending Collell et al 2017 \cite{collell})
          \item both language and vision to a common representation space
       \end{itemize} 

    \item
      Test jointly trained features versus pre-learned features when training on both modalities.
 \end{itemize} 

\section{Experimental Architectures}


Following the example set by Peng et al 2017 \cite{DBLP:journals/corr/abs-1710-05106} and by Wang et al \cite{acmr}, who seem to follow a similar approach to utilize an adversarial setting \cite{2014arXiv1406.2661G} for \textit{Cross-Modal Representation Learning}, we aim to experiment with different configurations and building blocks.

In both of the aforementioned articles we observe the usage of initial representations derived from a unimodal setting:

 \begin{itemize} 
  \item{Visual Embedding:} 4,096 dimension feature vector from the fc7 layer of a VGG-Net trained on ImageNet for both cases
  \item{Word Embeddings:} 
     \begin{itemize} 
      \item $n \times x$ matrix, with n the number of words and k the number of Word2Vec Features \cite{DBLP:journals/corr/MikolovSCCD13}
      \item Bag of Words vector with TF-IDF weighting

     \end{itemize} 
 \end{itemize} 

They both use fully connected layers to arrive to a common representation and use discriminators inter-modally and intra-modally. Yet the approaches differ in their implementations and also the acmr method uses a label predictor as an additional discriminator network.

I \DIFdelbegin \DIFdel{plan to experiment with architectures proposed for image to image transformations and apply them in a Cross-Modal setting. Namely Disco-GANs \mbox{%DIFAUXCMD
\cite{DBLP:journals/corr/KimCKLK17} }%DIFAUXCMD
and UNIT \mbox{%DIFAUXCMD
\cite{DBLP:journals/corr/LiuBK17} }%DIFAUXCMD
. Initially, the idea would be to imitate the abstract composition of the proposed architectures.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{I }\DIFdelend want to try two approaches. Firstly, following the above adversarial cross-modal paradigms, I want to try using the embeddings of pre-trained networks as input to these architectures. Alternatively, a more ambitious approach would be to try to train the networks while feeding them with raw image and sentences-describing-the-image data, for instance by using the MSCOCO dataset \cite{DBLP:journals/corr/LiuBK17}, and acquire the embeddings as a byproduct of the procedure. 

In the first approach, the task we would train upon is information retrieval, both Bi-modal and All-modal. \DIFaddbegin \DIFadd{Specifically, we want to measure the cross-modal retrieval performance in terms of mAP on the tested datasets (e.g. Wikipedia, Pascal, MSCOCO etc.). }\DIFaddend In the later, the goal is to train networks simultaneously for $Image \rightarrow Text$ generation \cite{Karpathy:2017:DVA:3069214.3069250} and $Text \rightarrow Image$ \cite{DBLP:journals/corr/ReedAYLSL16} in an adversarial setting\DIFaddbegin \DIFadd{.
}

\DIFadd{I plan to experiment with architectures proposed for image to image transformations and apply them in a Cross-Modal setting. Namely Disco-GANs \mbox{%DIFAUXCMD
\cite{DBLP:journals/corr/KimCKLK17} }%DIFAUXCMD
and UNIT \mbox{%DIFAUXCMD
\cite{DBLP:journals/corr/LiuBK17} }%DIFAUXCMD
. Initially, the idea would be to imitate the abstract composition of the proposed architectures. For instance, UNIT proposes a combination of Variational Auto-Encoders with GANs, and it would be interesting to test if the usage of VAE instead of Convolutional Auto-Encoders (as proposed by Peng et al \mbox{%DIFAUXCMD
\cite{DBLP:journals/corr/abs-1710-05106}}%DIFAUXCMD
) would prove beneficial to our task. The DISCO-GANs paper proposes a coupling of transformations procedures that use generators that take as input an image from one domain and transform it to an other. Drawing upon this idea, we can use a similar architecture with $image \rightarrow text$ and $text \rightarrow image$ generators as building blocks}\DIFaddend .


\section{Challenges}

 \begin{itemize} 
    \item GANs are computationally demanding to train 
    \item Will need access to GPUs (I don't know if that's a given)
 \end{itemize} 










\bibliography{references}{}
\bibliographystyle{ieeetr}

\end{document}
