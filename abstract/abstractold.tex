% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{cite}
\graphicspath{ {../media/} } 

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
%\renewcommand{\qedsymbol}{\filledbox}
 
\title{Integrating vision and language using generative models: a research proposal}
\author{Evangelos Ntavelis\\ %replace with your name
r0692337\\
} %if necessary, replace with your course title
 
\maketitle

\section{Introduction}

There is evidence that the nature of human memory is reconstructive (Vernon 2014 \cite{ACS}; Hawkins and Blakeslee 2007\cite{onInt}). That means that our neural system breaks down the stimuli it receives from the outside world into an internal representation and it recreates it when the need arises. Moreover, Barsalou\cite{doi:10.1146/annurev.psych.59.103006.093639} argues that ground cognition draws upon the senses. It's normal, thus, to aim to computationally recreate this multi-modal decomposition, coding and composition procedure.

In the previous years we can observe a lot of approaches tackling this problem using auto-encoders, both combining audio and visual features (Ngiam et al, 2011 \cite{conf/icml/NgiamKKNLN11}) and visual and text features (Silberer and Lapata 2014 \cite{silberer2014learning}). The encoders were fed with pre-learned features independently for both modalities, which were then combined \textit{a posteriori}. 

More recently, a generative family of models, Generative Adversarial Networks, have gained popularity. Among other things, they were used as an alternative method to design multi-modal embeddings in video hyperlinking with promising results (Vukotic et al 2017).

\section{Goals}

In the course of this thesis project we aim to test how Generative Adversarial Networks Architectures perform in tasks of lingo-visual representations. Specifically, we want to:

\begin{itemize}
    \item Gauge the ability of these architectures to map informatively:
      \begin{itemize}
        \item language to vision and back, (extending Collell et al 2017 \cite{collell})
          \item both language and vision to a common representation space
      \end{itemize}

    \item
      Test jointly trained features versus pre-learned features when training on both modalities.
\end{itemize}

\section{Experimental Architectures}


Following the example set by Peng et al 2017 \cite{DBLP:journals/corr/abs-1710-05106} and by Wang et al \cite{acmr}, who seem to follow a similar approach to utilize an adversarial setting \cite{2014arXiv1406.2661G} for \textit{Cross-Modal Representation Learning}, we aim to experiment with different configurations and building blocks.

In both of the aforementioned articles we observe the usage of initial representations derived from a unimodal setting:

\begin{itemize}
  \item{Visual Embedding:} 4,096 dimension feature vector from the fc7 layer of a VGG-Net trained on ImageNet for both cases
  \item{Word Embeddings:} 
    \begin{itemize}
      \item $n \times x$ matrix, with n the number of words and k the number of Word2Vec Features \cite{DBLP:journals/corr/MikolovSCCD13}
      \item Bag of Words vector with TF-IDF weighting

    \end{itemize}
\end{itemize}

They both use fully connected layers to arrive to a common representation and use discriminators inter-modally and intra-modally. Yet the approaches differ in their implementations and also the acmr method uses a label predictor as an additional discriminator network.

I plan to experiment with architectures proposed for image to image transformations and apply them in a Cross-Modal setting. Namely Disco-GANs \cite{DBLP:journals/corr/KimCKLK17} and UNIT \cite{DBLP:journals/corr/LiuBK17} . Initially, the idea would be to imitate the abstract composition of the proposed architectures.

I want to try two approaches. Firstly, following the above adversarial cross-modal paradigms, I want to try using the embeddings of pre-trained networks as input to these architectures. Alternatively, a more ambitious approach would be to try to train the networks while feeding them with raw image and sentences-describing-the-image data, for instance by using the MSCOCO dataset \cite{DBLP:journals/corr/LiuBK17}, and acquire the embeddings as a byproduct of the procedure. 

In the first approach, the task we would train upon is information retrieval, both Bi-modal and All-modal. In the later, the goal is to train networks simultaneously for $Image \rightarrow Text$ generation \cite{Karpathy:2017:DVA:3069214.3069250} and $Text \rightarrow Image$ \cite{DBLP:journals/corr/ReedAYLSL16} in an adversarial setting.

\section{Challenges}

\begin{itemize}
    \item GANs are computationally demanding to train 
    \item Will need access to GPUs (I don't know if that's a given)
\end{itemize}










\bibliography{references}{}
\bibliographystyle{ieeetr}

\end{document}
